{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Logistic_Regression",
     "Binary_Classification",
     "Derivatives"
    ]
   },
   "source": [
    "### Problem Statement\n",
    "- It is a binary classification.\n",
    "- Will focus on forward Pass and Backpropogation.\n",
    "- Will check different libraries solution for logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation Guide\n",
    "\n",
    "- n : dimension of features.\n",
    "- m : number of examples.\n",
    "- y : output label | result. (coloumnar representation) (1 * m)\n",
    "- y^ : probability of y = 0|1 given x , P(y=1|x) . Mathmatically : sigmoid of (W^t * x + b)\n",
    "- (x | y) : pair of input and output.\n",
    "- X : complete matrix to represent. (n rows and m coloumns )  different representation as taken Transpose  already. (n * m)\n",
    "- W : weights , it will be of n * 1 shape\n",
    "- b : bias term , numeric value\n",
    "- Dvar : derivative of final output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "This is applied to single training set.<br>\n",
    "<br>\n",
    "Following is as explaination of loss function.\n",
    "- If y == 1, second term is 0 and loss is just first term.\n",
    "- If y == 0, first term is 0 and loss is just second term.\n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "\\dot{y} & = \\sigma(W^t x + b ) \\\\\n",
    "\\sigma(z) & = 1/(1 + e^{-z}) \\\\\n",
    "\\end{align} \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "L(\\dot{y}, y) & = -( y * \\log(\\dot{y}) + (1-y) * \\log(1 - \\dot{y}) )\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function  \n",
    "\n",
    "Applied to All the training example.\n",
    "\n",
    "\\begin{equation*}\n",
    "Cost  = 1/m \\sum_{k=1}^m L( \\dot{y}^i, y^i )\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Decent \n",
    "\n",
    "- Since above cost function is the convex problem so it will have single minima.\n",
    "- Since this is convex problem , no matter with what weight we initialize we will reach the same minima.\n",
    "\n",
    "Repeat : { W = W - alpha * derivative of Cost Function. }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives\n",
    "\n",
    "Derivative can be consider as the the slope of the fucntion with respect to variable which we chanhe to calculate the slope.\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "Partial Derivative  =  \\frac{\\partial f(a)}{\\partial a} \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "In this , we are calculating the slope of the function f(a) by changing the value of \"a\" towards right by very very small amount. <br>\n",
    "\n",
    "When\n",
    "\\begin{equation*}\n",
    "f(a) = 3a\n",
    "\\end{equation*}\n",
    "\n",
    "Slope of the function will be 3 for every value if a , in this case slope remain same as it is the equation of straigh line, there can be different slope at different value of \"a\".\n",
    "\n",
    "For Example for :\n",
    "\n",
    "\\begin{equation*}\n",
    "f(a) = a^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Slope is different at different point of a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Important Derivatives\n",
    "\n",
    "\\begin{align}\n",
    "function :f(a) = \\log(a) \\\\ \n",
    "Derivative : \\frac{\\partial f(a)}{\\partial a}   =  1/a\n",
    "\\end{align}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "function : L(\\dot{y}, y) & = -( y * \\log(\\dot{y}) + (1-y) * \\log(1 - \\dot{y}) ) \\\\\n",
    "Derivative = -y/\\dot{y} + (1-y)/(1-\\dot{a})\n",
    "\\end{align}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "function :a = \\sigma(z) \\\\ \n",
    "Derivative : a * (1-a)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Maths For Single Training Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Algorith Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "helper_function"
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0.0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b ) # compute activation\n",
    "    cost = 1/m *np.sum((-((Y * np.log(A)) + ((1-Y) * np.log(1-A)))))                                 # compute cost\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = 1/m * np.dot(X, (A - Y).T)\n",
    "    db = 1/m * np.sum(A - Y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b )\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[0,i] > 0.5 :\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "                \n",
    "        pass\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "helper_function"
    ]
   },
   "outputs": [],
   "source": [
    "def logictic_loss(a, y):\n",
    "    return  -((y * np.log(a)) + ((1-y) * np.log(1-a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "Vectorization",
     "Forward_Pass",
     "Backward_Pass"
    ]
   },
   "outputs": [],
   "source": [
    "# n : number of features, m : number of examples\n",
    "# X input - shape (n, m)\n",
    "# w weights - shape (n, 1)\n",
    "# y output - shape ()\n",
    "# bias - shape (1, m) ? or just real number\n",
    "# alpha - learning rate , real number\n",
    "\n",
    "# expected shape of z = (1, m), d_w = (n, 1), \n",
    "def logistic_regression(X, w, b, y, alpha, debug = False):\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    ## forward propogation\n",
    "    if debug:\n",
    "        print('Shape of X', X.shape)\n",
    "        print('Shape of w', w.shape)\n",
    "        print('Shape of b', b.shape)\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    if debug:\n",
    "        print('Shape of z', z.shape)\n",
    "    # predicted value    \n",
    "    A = sigmoid(Z)    \n",
    "    if debug:\n",
    "        print('Shape of a', a.shape)  \n",
    "    #loss can be calculated here\n",
    "    loss = logictic_loss(A, y)\n",
    "    cost = 1/m * np.sum(loss)\n",
    "    ## backward propogation\n",
    "    d_Z = A - y\n",
    "    if debug:\n",
    "        print('Shape of d_z', d_Z.shape)     \n",
    "    d_w = 1/m * np.dot(X, d_Z.T)\n",
    "    if debug:\n",
    "        print('Shape of d_w', d_w.shape)     \n",
    "    d_b = 1/m * np.sum(d_Z)\n",
    "    return d_w, d_b, cost\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Test Logictic Regression Function\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = np.zeros((X_train.shape[0], 1)), 0.0\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b , X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy things to remember\n",
    "\n",
    "- avoid for loops when ever possible , use vectorization if possible.\n",
    "- axis = 0 , vertical.\n",
    "- axis = 1, horizaontal.\n",
    "- preffer row vector or coloumn vector , and avoid rank 1 vector.\n",
    "- Row Vector -> (1, n)\n",
    "- Columnar Vector -> (n,1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Broadcasting\n",
    "\n",
    "- Actions between two matrixs like [addition, subtractions, multipy, divide].\n",
    "- first matrix of size N * M\n",
    "- if secondand matrix is of 1 * M , it will be converted to N * M, eg [1,2,3] converted to [[1,2,3], [1,2,3]] for N = 2\n",
    "- if secondand matrix is of N * 1, it will be converted to N * M, eg [[1], [2], [3]] converted to [[1,1], [2, 2], [3,3]] if M = 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Cost Function is choose for logictic Regression\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Guide\n",
    "\n",
    "- What does the neural networks compute : Simple Linear function with activation.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some other loss functions\n",
    "\n",
    "\n",
    "$$\\begin{align*} & L1 Loss  & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}$$\n",
    "\n",
    "\n",
    "$$\\begin{align*} & L2 Loss & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
